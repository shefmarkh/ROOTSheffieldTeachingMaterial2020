{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again we use the \"!\" command to specify we want to execute commands via the terminal, outside python\n",
    "#rootls is an example of a root command line tool: \n",
    "#https://root.cern/manual/storing_root_objects/#root-command-line-tools\n",
    "#Allows to run common operations from the linux/mac terminal, without running ROOT or python directly.\n",
    "! rootls ../Lecture1/tree2.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do the same in python we would have to type:\n",
    "from ROOT import TFile\n",
    "myFile = TFile.Open(\"../Lecture1/tree2.root\")\n",
    "#Note the output is a bit more detailed though\n",
    "myFile.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"make\" is a standard command to build an executable, which you can run. An executable can be thought of as a \n",
    "#translation of your computer code into something the computer can understand. When using python this happens in\n",
    "#real time, behind the scenes.\n",
    "#The \"!\" tells Jupyter to run the command in the underlying terminal.\n",
    "! make \n",
    "#You can see the commands \"make\" actually runs printed below.\n",
    "#Lots of options passed to g++ (don't worry about these) - the important part is it creates library files (*.d,*.o)\n",
    "#and eventually an executable which you run (\"myROOTCommand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the command in the terminal, and we see what it does.\n",
    "! ./myROOTCommand ../Lecture1/tree2.root t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also make a python shortcut command!\n",
    "#Note in linux you can skip the \"python3\" part (needed in Jupyter on mybinder!)\n",
    "! python3 ./myROOTCommand.py ../Lecture1/tree2.root t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1\n",
    "#Make your own python shortcut command which draws an arbitrary variable from the TTree in tree2.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets use uproot - note there is no c++ version! Python only.\n",
    "import uproot\n",
    "#Load the TTree from the ATLAS open data (note the miniTree variable is NOT of type TTree - we are now\n",
    "#in the uproot ecosystem and not in the ROOT ecosystem\n",
    "miniTreeZmumu = uproot.open(\"https://atlas-opendata.web.cern.ch/Legacy13TeV/2lep/MC/mc_364100.Zmumu_PTV0_70_CVetoBVeto.2lep.root\")[\"mini\"]\n",
    "#Lets print the variables in the TTree\n",
    "miniTreeZmumu.keys()\n",
    "#note these variables are defined at https://atlas-opendata.web.cern.ch/docs/datasets/dataset13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose which variables we want to use\n",
    "#entry_stop restricts the number of entries to use (just done to make example run faster...)\n",
    "#library set to 'pd' gives us a pandas dataframe\n",
    "df_Zmumu = miniTreeZmumu.arrays([\"lep_n\",\"jet_n\"],library='pd',entry_stop=1000)\n",
    "df_Zmumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In my notebook, had to run this cell twice to get the histograms to display...\n",
    "df_Zmumu.hist(\"jet_n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniTreeTTbar = uproot.open(\"https://atlas-opendata.web.cern.ch/Legacy13TeV/2lep/MC/mc_410000.ttbar_lep.2lep.root\")[\"mini\"]\n",
    "df_ttbar = miniTreeTTbar.arrays([\"jet_n\",\"jet_pt\"],library='pd',entry_stop=1000)\n",
    "#jet_pt is an \"awkward array\" (i.e of variable size), stored as a series\n",
    "#so we have to use this procedure to flatten it to a 1-D array that \n",
    "#the hist function would use.\n",
    "#First we extract the jet_pt as an array of lists (one list per event)\n",
    "jet_pt = (df_ttbar['jet_pt']).to_numpy()\n",
    "#Then we flatten this into one array of all jet pt\n",
    "import numpy as np\n",
    "flat_jet_pt = np.concatenate(jet_pt)\n",
    "import matplotlib.pylab as plt\n",
    "#Make histogram using pylab hist function\n",
    "jet_ptHist = plt.hist(flat_jet_pt,bins=10,range=[0,100000])\n",
    "#in python prompt you need to run plt.show() at this step, not needed in Jupyter\n",
    "plt.title(\"Jet Pt\")\n",
    "plt.xlabel(\"Jet Pt (MeV)\")\n",
    "plt.ylabel(\"Number of Jets\")\n",
    "#Other variable types can be histogrammed directly using the pandas array\n",
    "df_ttbar.hist(\"jet_n\",bins=10)\n",
    "plt.title(\"Number of Jets\")\n",
    "plt.xlabel(\"Number of Jets\")\n",
    "plt.ylabel(\"Number of Events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2\n",
    "#Try making some histograms of other quantities in the open data.\n",
    "#Can you reproduce plots we saw in earlier Lectures with .../Lecture1/tree2.root using uproot and matplotlib?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefine ttbar array with same variables as Zmumu array\n",
    "df_ttbar = miniTreeTTbar.arrays([\"lep_n\",\"jet_n\"],library='pd',entry_stop=1000)\n",
    "df_Zmumu=df_Zmumu.assign(signal=1)\n",
    "df_ttbar=df_ttbar.assign(signal=0)\n",
    "#Create a list of the two pandas data frames\n",
    "df_list = [df_Zmumu,df_ttbar]\n",
    "#Convert this new list to one pandas data frame\n",
    "import pandas\n",
    "df_all = pandas.concat(df_list)\n",
    "#In case we only have integers, make sure they are treated as floats (ML software usually requires variables to be floats)\n",
    "df_all = df_all.astype('float64')\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_all.hist(\"signal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_all.values\n",
    "X = dataset[:,0:2]\n",
    "Y = dataset[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML prefers to have variables distributed in a specific way\n",
    "#Here we require them to have mean of zero and standard deviation of 1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as ks\n",
    "keras_model = ks.Sequential()\n",
    "keras_model.add(ks.layers.Dense(12,activation='relu'))\n",
    "keras_model.add(ks.layers.Dense(8, activation='relu'))\n",
    "keras_model.add(ks.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "keras_model.fit(X,Y,epochs=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = keras_model.predict(X)\n",
    "\n",
    "#Change our predicted probability to a binary 0 or 1\n",
    "def final_predictions(predictions):\n",
    "    finalPredictions = np.zeros((len(predictions),))\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] > 0.5:\n",
    "            finalPredictions[i] = 1\n",
    "        else:\n",
    "            finalPredictions[i] = 0\n",
    "\n",
    "    return finalPredictions\n",
    "\n",
    "#Count how often our predictin matches the true signal value\n",
    "def count_correct_predictions(predictions, Y):\n",
    "    nCorrect = 0\n",
    "    for i in range(len(predictions)-1):\n",
    "        if predictions[i] == Y[i]:\n",
    "            nCorrect += 1\n",
    "    return nCorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalPredictions = final_predictions(predictions)\n",
    "print(\"Number of correct predictions are \", count_correct_predictions(finalPredictions, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the same model using Google DeepMinds Sonnet software\n",
    "import sonnet as snt\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model():\n",
    "  model = snt.Sequential([\n",
    "      snt.Linear(12),\n",
    "      tf.nn.relu,\n",
    "      snt.Linear(8),\n",
    "      tf.nn.relu,\n",
    "      snt.Linear(1),\n",
    "      tf.nn.sigmoid\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build our model\n",
    "sonnet_model = build_model()\n",
    "\n",
    "#Define a function to train our model\n",
    "def train_model(model, X, Y, epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    for epoch in range(epochs):        \n",
    "        for i in range(0, len(X), batch_size):\n",
    "            batch_X = X[i:i+batch_size]\n",
    "            batch_y = Y[i:i+batch_size]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_X)\n",
    "                loss = loss_fn(batch_y, predictions)\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train this sonnet model and check its performance\n",
    "\n",
    "train_model(sonnet_model, X,Y)\n",
    "predictions = sonnet_model(X)\n",
    "print(\"sonnet predictions are: \", predictions)\n",
    "\n",
    "finalPredictions = final_predictions(predictions)\n",
    "print(\"Number of correct predictions are \", count_correct_predictions(finalPredictions, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
